{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsPmTeR219Hb"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/likelian/source-separation/blob/main/Wave-U-Net-Pytorch/train.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jBANF_h19Hf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main(args):\n",
        "    #torch.backends.cudnn.benchmark=True # This makes dilated conv much faster for CuDNN 7.5\n",
        "\n",
        "    # MODEL\n",
        "    num_features = [args.features*i for i in range(1, args.levels+1)] if args.feature_growth == \"add\" else \\\n",
        "                   [args.features*2**i for i in range(0, args.levels)]\n",
        "    target_outputs = int(args.output_size * args.sr)\n",
        "    model = Waveunet(args.channels, num_features, args.channels, args.instruments, kernel_size=args.kernel_size,\n",
        "                     target_output_size=target_outputs, depth=args.depth, strides=args.strides,\n",
        "                     conv_type=args.conv_type, res=args.res, separate=args.separate)\n",
        "\n",
        "    if args.cuda:\n",
        "        model = model_utils.DataParallel(model)\n",
        "        print(\"move model to gpu\")\n",
        "        model.cuda()\n",
        "\n",
        "    print('model: ', model)\n",
        "    print('parameter count: ', str(sum(p.numel() for p in model.parameters())))\n",
        "\n",
        "    writer = SummaryWriter(args.log_dir)\n",
        "\n",
        "    ### DATASET\n",
        "    musdb = get_musdb_folds(args.dataset_dir)\n",
        "    # musdb = get_musdb_folds(args.dataset_dir, 0)\n",
        "    # If not data augmentation, at least crop targets to fit model output shape\n",
        "    crop_func = partial(crop_targets, shapes=model.shapes)\n",
        "    # Data augmentation function for training\n",
        "    augment_func = partial(random_amplify, shapes=model.shapes, min=0.7, max=1.0)\n",
        "    train_data = SeparationDataset(musdb, \"train\", args.instruments, args.sr, args.channels, model.shapes, True, args.hdf_dir, audio_transform=augment_func)\n",
        "    val_data = SeparationDataset(musdb, \"val\", args.instruments, args.sr, args.channels, model.shapes, False, args.hdf_dir, audio_transform=crop_func)\n",
        "    test_data = SeparationDataset(musdb, \"test\", args.instruments, args.sr, args.channels, model.shapes, False, args.hdf_dir, audio_transform=crop_func)\n",
        "\n",
        "    dataloader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, worker_init_fn=utils.worker_init_fn)\n",
        "\n",
        "    ##### TRAINING ####\n",
        "\n",
        "    # Set up the loss function\n",
        "    if args.loss == \"L1\":\n",
        "        criterion = nn.L1Loss()\n",
        "    elif args.loss == \"L2\":\n",
        "        criterion = nn.MSELoss()\n",
        "    else:\n",
        "        raise NotImplementedError(\"Couldn't find this loss!\")\n",
        "\n",
        "    # Set up optimiser\n",
        "    optimizer = Adam(params=model.parameters(), lr=args.lr)\n",
        "\n",
        "    # Set up training state dict that will also be saved into checkpoints\n",
        "    state = {\"step\" : 0,\n",
        "             \"worse_epochs\" : 0,\n",
        "             \"epochs\" : 0,\n",
        "             \"best_loss\" : np.Inf}\n",
        "\n",
        "    # LOAD MODEL CHECKPOINT IF DESIRED\n",
        "    if args.load_model is not None:\n",
        "        print(\"Continuing training full model from checkpoint \" + str(args.load_model))\n",
        "        state = model_utils.load_model(model, optimizer, args.load_model, args.cuda)\n",
        "\n",
        "    print('TRAINING START')\n",
        "    while state[\"worse_epochs\"] < args.patience:\n",
        "        print(\"Training one epoch from iteration \" + str(state[\"step\"]))\n",
        "        avg_time = 0.\n",
        "        model.train()\n",
        "        with tqdm(total=len(train_data) // args.batch_size) as pbar:\n",
        "            np.random.seed()\n",
        "            for example_num, (x, targets) in enumerate(dataloader):\n",
        "                if args.cuda:\n",
        "                    x = x.cuda()\n",
        "                    for k in list(targets.keys()):\n",
        "                        targets[k] = targets[k].cuda()\n",
        "\n",
        "                t = time.time()\n",
        "\n",
        "                # Set LR for this iteration\n",
        "                utils.set_cyclic_lr(optimizer, example_num, len(train_data) // args.batch_size, args.cycles, args.min_lr, args.lr)\n",
        "                writer.add_scalar(\"lr\", utils.get_lr(optimizer), state[\"step\"])\n",
        "\n",
        "                # Compute loss for each instrument/model\n",
        "                optimizer.zero_grad()\n",
        "                outputs, avg_loss = model_utils.compute_loss(model, x, targets, criterion, compute_grad=True)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                state[\"step\"] += 1\n",
        "\n",
        "                t = time.time() - t\n",
        "                avg_time += (1. / float(example_num + 1)) * (t - avg_time)\n",
        "\n",
        "                writer.add_scalar(\"train_loss\", avg_loss, state[\"step\"])\n",
        "\n",
        "                if example_num % args.example_freq == 0:\n",
        "                    input_centre = torch.mean(x[0, :, model.shapes[\"output_start_frame\"]:model.shapes[\"output_end_frame\"]], 0) # Stereo not supported for logs yet\n",
        "                    writer.add_audio(\"input\", input_centre, state[\"step\"], sample_rate=args.sr)\n",
        "\n",
        "                    for inst in outputs.keys():\n",
        "                        writer.add_audio(inst + \"_pred\", torch.mean(outputs[inst][0], 0), state[\"step\"], sample_rate=args.sr)\n",
        "                        writer.add_audio(inst + \"_target\", torch.mean(targets[inst][0], 0), state[\"step\"], sample_rate=args.sr)\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "        # VALIDATE\n",
        "        val_loss = validate(args, model, criterion, val_data)\n",
        "        print(\"VALIDATION FINISHED: LOSS: \" + str(val_loss))\n",
        "        writer.add_scalar(\"val_loss\", val_loss, state[\"step\"])\n",
        "\n",
        "        # EARLY STOPPING CHECK\n",
        "        checkpoint_path = os.path.join(args.checkpoint_dir, \"checkpoint_\" + str(state[\"step\"]))\n",
        "        if val_loss >= state[\"best_loss\"]:\n",
        "            state[\"worse_epochs\"] += 1\n",
        "        else:\n",
        "            print(\"MODEL IMPROVED ON VALIDATION SET!\")\n",
        "            state[\"worse_epochs\"] = 0\n",
        "            state[\"best_loss\"] = val_loss\n",
        "            state[\"best_checkpoint\"] = checkpoint_path\n",
        "\n",
        "        # CHECKPOINT\n",
        "        print(\"Saving model...\")\n",
        "        model_utils.save_model(model, optimizer, state, checkpoint_path)\n",
        "\n",
        "        state[\"epochs\"] += 1\n",
        "\n",
        "    #### TESTING ####\n",
        "    # Test loss\n",
        "    print(\"TESTING\")\n",
        "\n",
        "    # Load best model based on validation loss\n",
        "    state = model_utils.load_model(model, None, state[\"best_checkpoint\"], args.cuda)\n",
        "    test_loss = validate(args, model, criterion, test_data)\n",
        "    print(\"TEST FINISHED: LOSS: \" + str(test_loss))\n",
        "    writer.add_scalar(\"test_loss\", test_loss, state[\"step\"])\n",
        "\n",
        "    # Mir_eval metrics\n",
        "    test_metrics = evaluate(args, musdb[\"test\"], model, args.instruments)\n",
        "\n",
        "    # Dump all metrics results into pickle file for later analysis if needed\n",
        "    with open(os.path.join(args.checkpoint_dir, \"results.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(test_metrics, f)\n",
        "\n",
        "    # Write most important metrics into Tensorboard log\n",
        "    avg_SDRs = {inst : np.mean([np.nanmean(song[inst][\"SDR\"]) for song in test_metrics]) for inst in args.instruments}\n",
        "    avg_SIRs = {inst : np.mean([np.nanmean(song[inst][\"SIR\"]) for song in test_metrics]) for inst in args.instruments}\n",
        "    for inst in args.instruments:\n",
        "        writer.add_scalar(\"test_SDR_\" + inst, avg_SDRs[inst], state[\"step\"])\n",
        "        writer.add_scalar(\"test_SIR_\" + inst, avg_SIRs[inst], state[\"step\"])\n",
        "    overall_SDR = np.mean([v for v in avg_SDRs.values()])\n",
        "    writer.add_scalar(\"test_SDR\", overall_SDR)\n",
        "    print(\"SDR: \" + str(overall_SDR))\n",
        "\n",
        "    writer.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
